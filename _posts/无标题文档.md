---
title: 'Blog Post number 2'
date: 2025-1-6
permalink: /posts/2013/08/blog-post/
tags:
  - cool posts
  - category1
  - category2
---

这就进入了大模型调教的真正“炼丹术”核心部分：**PPO在RLHF中的具体训练流程**。  
它名义上是强化学习，但实际上是一种“基于奖励反馈的有监督微调”——一个表面是RL，骨子里是稳健优化的混合体。

---

## 一、背景：RLHF 三阶段
PPO只是RLHF的第三步。整个过程是：

1. **Supervised Fine-Tuning (SFT)**：  
让模型学会基本对齐人类语言（模仿人类书写风格）。  
输出：一个初步可用的语言模型（policy π₀）。
2. **Reward Model (RM) 训练**：  
用人类偏好数据训练一个评分器：  
给同一指令的两个回答（A、B），  
人标注哪一个更好，训练reward model使 `R(A) > R(B)`。  
输出：一个能对生成文本打分的 reward model。
3. **Reinforcement Learning with PPO**：  
用 reward model 给出的分数，微调 π₀，得到强化后的 π₁。  
核心优化目标就是 **PPO loss**。

---

## 二、核心思路
PPO 训练语言模型的目标是让它生成的内容得到更高的奖励分数（由RM给出），  
同时不能偏离原始SFT模型太远（避免“崩坏”）。

公式化地：  
$ L(\pi) = \mathbb{E}_{x, y \sim \pi} \left[ r(y) - \beta , \text{KL}(\pi(\cdot|x) || \pi_0(\cdot|x)) \right] $  
  


+ ( r(y) )：reward model 对生成文本的打分
+ ( \beta )：控制与原始模型的偏离程度
+ KL项：约束模型不要乱跑（防止输出不稳定或过度自信）

然后用PPO算法来优化上面的期望。

---

## 三、训练流程分解（逐步）
### 1. 数据采样
+ 从人类编写的prompt池中采样一个输入 ( x )；
+ 当前policy（语言模型）πθ生成一个响应 ( y = \text{sample}(\pi_\theta(\cdot|x)) )。

这就是一个“episode”——虽然只有一步，但被当作一个回合处理。

---

### 2. 奖励计算
+ 用 reward model ( R_\phi(x, y) ) 打分；
+ 加上 KL 惩罚项（鼓励不要离 π₀ 太远）：  
$$ r_t = R_\phi(x, y) - \beta , \text{KL}[\pi_\theta(y|x) || \pi_0(y|x)] $$  
得到每个样本的“奖励” ( r_t )。

---

### 3. 计算 Advantage
用 value head（Vθ）预测该prompt下的期望奖励：  
[  
A_t = r_t - V_\theta(x)  
]  
这个 advantage 表示“比我预期的好多少”，  
它被用来指导更新方向。

---

### 4. PPO目标
核心优化目标是：  
[  
L^{CLIP}(\theta) = \mathbb{E}_t \left[  
\min\left(  
r_t(\theta) A_t,,  
\text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) A_t  
\right)  
\right]  
]

其中：  
[  
r_t(\theta) = \frac{\pi_\theta(y_t|x_t)}{\pi_{\theta_{\text{old}}}(y_t|x_t)}  
]  
它衡量更新前后同一个生成的概率变化。

这一步是PPO最重要的“稳定机制”：  
即便你梯度想走太猛，它也会“夹”回去。

---

### 5. Value Loss
同时更新 value head：  
[  
L<sup>{value} = (V_\theta(x_t) - r_t)</sup>2  
]

---

### 6. 最终总损失
[  
L_{\text{total}} = -L<sup>{CLIP} + c_1 L</sup>{value} - c_2 , \text{Entropy}  
]  
其中：

+ `c1` 控制 value loss 权重；
+ `c2` 控制熵正则，保持探索。

---

## 四、训练伪代码（接近实际实现）
```python
for batch in dataloader:
    prompts = batch["prompt"]
    old_log_probs, values, responses = [], [], []
    
    # 1. 生成
    for prompt in prompts:
        response, logprobs = model.generate(prompt)
        reward = reward_model(prompt, response)
        kl_penalty = beta * (logprobs - ref_model.log_probs(prompt, response))
        rewards.append(reward - kl_penalty)
    
    # 2. 计算 advantage
    advantages = rewards - values.detach()

    # 3. 计算 PPO loss
    ratio = torch.exp(new_log_probs - old_log_probs)
    clip_loss = torch.mean(torch.min(
        ratio * advantages,
        torch.clamp(ratio, 1 - eps, 1 + eps) * advantages
    ))
    
    # 4. Value loss
    value_loss = F.mse_loss(values, rewards)
    
    # 5. 合并并反向传播
    loss = -clip_loss + c1 * value_loss - c2 * entropy
    loss.backward()
    optimizer.step()
```

---

## 五、一些工程要点
1. **批量生成时异步采样**：通常用 `generate()` 的采样模式（非贪婪）；
2. **梯度累积**：因为显存吃紧；
3. **KL控制 β 自适应调整**：有的实现用控制目标 KL ≈ 目标值的方式；
4. **多GPU训练**：policy与reward model分布式；
5. **每隔几个step重新采样**：避免过拟合同一批prompt。

---

## 六、直觉理解
+ PPO阶段 ≈ 一种“人类指导的奖励驱动微调”；
+ 它让模型学会**倾向于人类喜欢的答案**；
+ KL项让它**不忘之前的语言能力**；
+ value head只是为了让更新**更平稳**。

---

## 七、扩展：为什么后来大家弃用PPO？
因为：

+ 它需要 reward model；
+ 它要 rollout；
+ 它更新不稳；
+ 它成本高。

所以新方法（如 DPO、KTO、IPO）用更简洁的目标函数直接从偏好对中学习，不再真的跑强化学习。

---

一句话总结：

> 在RLHF的PPO阶段，大模型被当作policy，reward model当作环境反馈，value head当作baseline。  
它其实不是在“探索环境”，而是在**微调模型，让它的输出更合人意**。
>

---

如果你想更深入，我可以给你对比下 **PPO（RLHF） vs DPO（直接偏好优化）** 的公式与实际行为差异，让你看到后者为什么更高效、易收敛。是否继续讲？

